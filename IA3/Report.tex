\documentclass[letterpaper,10pt]{article}
\title{Assignment 3 Report - KNN and Decision Trees}
\author{Helena Bales and Natalie Suderman\\ \\ CS343 - Spring 2017}
\usepackage[pdftex]{graphicx}
\usepackage{tikz}
\usepackage{float}

\parindent = 0.0 in
\parskip = 0.1 in

\begin{document}
\maketitle

\tableofcontents
\clearpage

\section{Introduction}

\section{Model Selection for KNN}

\subsection{Implementing the K-Nearest Neighbors Algorithm}
The K-Nearest Neighbors Algorithm is implemented in Python in the file knn.py

In implementing the KNN algorithm, we first read in the datat from the provided csv. The data is 
in the form of an Nx31 dimentional matrix. The dataset constitutes 30 features with the first 
value being the true class label. For example:\\

1, 19.59, 25, 127.7, 1191, 0.1032, 0.09871, 0.1655, 0.09063, ..., 0.2293, 0.06091\\

In this case, the true class value is 1, meaning that it is malignant. If it were benign, the true
 class value would be -1.

The implementation reads in each line of the csv, removes the first value as the true class value 
then uses the following 30 features to determine the distance between data points. Before the 
distance between two points can be calculated, the scale for each feature must be determined and 
the feature scaled to be all in the same range (for example [0, 1]).

We can use the Euclidian or straight line distance to determine the distance between two points 
\(x\) and \(x_i\) with m features.

\[ x = \left[ \begin{array}{c} x_1 \\ \vdots \\ x_m \end{array} \right], x_i = \left[ \begin{array}{c} x_{i1} \\ \vdots \\ x_{im} \end{array} \right] \]
\[ D(x,x_i) = ||x - x_i|| \]
\[ D(x,x_i) = \sqrt{(x - x_i)^T(x - x_i)} \]
\[ D(x,x_i) = \sqrt{\sum_{j=1,...,m} (x_j - x_{ij})^2} \]

For each line in the csv, its k nearest neighbors will be used to determine its value.\\

\subsection{Evaluation of Error with Respect to K}
\subsubsection{Computed Training Error}
\subsubsection{Computed Leave-One-Out Cross-Validation Error}
\subsubsection{Number of Errors on Provided Test Data}
\subsubsection{Plot of Errors as a Function of K}

\subsection{Evaluation of Different Measures of Error and Model Selection}
\subsubsection{Evaluation of Measures of Error}
\subsubsection{Model Selection}

\section{Decision Tree}
\subsection{Implementing Learning Decision Tree Stump}
Best feature: 23
Best Split value: 100.25
Information Gain: .4272
Error: .3556
Test Error: .3908 
Base Branch Label: -1.0
Left Branch Label: -1.0
Right Branch Label: 1.0
\subsection{Implementing Top-Down Greedy Induction for Learning Decision Tree}

Total training error: \newline
0.557802806034 \newline
Total testing error: \newline
0.645376637757 \newline

Learned Decision Tree: \newline
values are ordered: best feature, split value, and direction of split 
 23 100.25 b \newline
        28 0.11857 l \newline
                21 12.005 l \newline
                        21 20.985 l \newline
                                12 3.243 l \newline
                                        1 12.785 r \newline
                                21 12.005 r \newline
                                        3 74.73 l \newline
                                        1 11.68 r \newline
                        1 12.58 r \newline
                                1 15.855 l \newline
                                1 15.92 r \newline
                                        1 16.065 r \newline
                28 0.064375 r \newline
                        28 0.1667 l \newline
                                8 0.0914 l \newline
                                        8 0.070085 r \newline
                                28 0.1668 r \newline
                                        1 16.4 l \newline
                                        28 0.088635 r \newline
                        1 13.28 r \newline
                                1 16.375 l \newline
                                        1 14.874 r \newline
                                1 12.57 r \newline
                                        1 13.535 l \newline
                                        1 17.635 r \newline
        23 129.735 r \newline
                21 21.17 l \newline
                        1 11.6635 l \newline
                                1 11.6635 l \newline
                                        1 11.6635 l \newline
                        21 11.2865 r \newline
                                21 11.0465 l \newline
                                        8 0.04211 l \newline
                                1 16.63 r \newline
                28 0.07791 r \newline
                        24 1119.6 l \newline
                                22 24.825 l \newline
                                        1 17.02 l \newline
                                        22 23.65 r \newline
                                21 11.99 r \newline
                                        21 15.935 l \newline
                                        1 13.865 r \newline
                        23 97.855 r \newline
                                21 16.77 l \newline
                                        1 13.625 l \newline
                                        1 12.555 r \newline
                                6 0.10433 r \newline
                                        1 15.375 l \newline
                                        6 0.106685 r \newline


\end{document}
